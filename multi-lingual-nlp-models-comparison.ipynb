{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow==2.12.0\n","metadata":{"execution":{"iopub.status.busy":"2023-08-14T16:43:46.338700Z","iopub.execute_input":"2023-08-14T16:43:46.339114Z","iopub.status.idle":"2023-08-14T16:43:51.230038Z","shell.execute_reply.started":"2023-08-14T16:43:46.339080Z","shell.execute_reply":"2023-08-14T16:43:51.229032Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.8/site-packages (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (2.3.0)\nRequirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (3.3.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (1.16.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (0.2.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (1.14.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (4.23.4)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (1.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (23.1)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (23.5.26)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (2.12.3)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (1.56.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (16.0.0)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (2.12.0)\nRequirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (0.4.6)\nRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (3.9.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (4.7.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (57.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (0.32.0)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.12.0) (0.4.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.40.0)\nRequirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.10.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.3.6)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.21.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.31.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.3.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\nRequirement already satisfied: urllib3<2.0 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.26.16)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (6.8.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.3)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.15.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.5.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# Matplotlib Inline\n%matplotlib inline\n\n# Import Modules\nimport gc\nimport random\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\n# import seaborn as sns\nimport tensorflow as tf\nfrom typing import Tuple\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\n# Install Transformers\n!pip install transformers==4.28.1\nfrom transformers import (TFGPT2Model, \n                          TFMBartModel,\n                          TFBertForSequenceClassification,\n                          TFDistilBertForSequenceClassification,\n                          TFXLMRobertaForSequenceClassification,\n                          TFMT5ForConditionalGeneration,\n                          TFT5ForConditionalGeneration,\n                          T5Tokenizer,\n                          AutoTokenizer,\n                          AutoConfig,\n                         TFBertModel)","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"papermill":{"duration":25.563841,"end_time":"2021-12-07T22:16:17.996971","exception":false,"start_time":"2021-12-07T22:15:52.43313","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-14T17:27:24.716543Z","iopub.execute_input":"2023-08-14T17:27:24.717798Z","iopub.status.idle":"2023-08-14T17:27:37.488910Z","shell.execute_reply.started":"2023-08-14T17:27:24.717750Z","shell.execute_reply":"2023-08-14T17:27:37.487622Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.28.1 in /usr/local/lib/python3.8/site-packages (4.28.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers==4.28.1) (4.65.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/site-packages (from transformers==4.28.1) (2023.8.8)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers==4.28.1) (6.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers==4.28.1) (23.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/site-packages (from transformers==4.28.1) (0.16.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers==4.28.1) (2.31.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers==4.28.1) (1.23.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers==4.28.1) (3.12.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/site-packages (from transformers==4.28.1) (0.13.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.7.1)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (2023.6.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.28.1) (1.26.16)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.28.1) (3.2.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.28.1) (2023.5.7)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers==4.28.1) (3.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# Configure Strategy. Assume TPU...if not set default for GPU/CPU\ntpu = None\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    \n# Seeds\ndef set_seeds(seed: int)->None:\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed) \n    \n# Generic Constants\nMAX_LEN = 512\nTEST_SIZE = 0.2\nLR = 0.00002\nVERBOSE = 1\nSEED = 1000\nset_seeds(SEED)\n\n# Set Autotune\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# Set Batch Size\nBASE_BATCH_SIZE = 4         # Modify to match your GPU card.\nif tpu is not None:         \n    BASE_BATCH_SIZE = 8     # TPU v2 or up...\nBATCH_SIZE = BASE_BATCH_SIZE * strategy.num_replicas_in_sync","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"papermill":{"duration":6.323561,"end_time":"2021-12-07T22:16:24.353517","exception":false,"start_time":"2021-12-07T22:16:18.029956","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-14T17:27:54.478657Z","iopub.execute_input":"2023-08-14T17:27:54.479087Z","iopub.status.idle":"2023-08-14T17:28:03.199722Z","shell.execute_reply.started":"2023-08-14T17:27:54.479055Z","shell.execute_reply":"2023-08-14T17:28:03.198500Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"To maximize the reproducibility for each model run we will use the same Seed, Batch Size and Learning Rate.","metadata":{"papermill":{"duration":0.03116,"end_time":"2021-12-07T22:16:24.416258","exception":false,"start_time":"2021-12-07T22:16:24.385098","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Summary\nprint(f'Seed: {SEED}')\nprint(f'Replica Count: {strategy.num_replicas_in_sync}')\nprint(f'Batch Size: {BATCH_SIZE}')\nprint(f'Learning Rate: {LR}')","metadata":{"papermill":{"duration":0.041088,"end_time":"2021-12-07T22:16:24.488946","exception":false,"start_time":"2021-12-07T22:16:24.447858","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-14T17:28:11.007258Z","iopub.execute_input":"2023-08-14T17:28:11.007652Z","iopub.status.idle":"2023-08-14T17:28:11.014200Z","shell.execute_reply.started":"2023-08-14T17:28:11.007622Z","shell.execute_reply":"2023-08-14T17:28:11.013000Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Seed: 1000\nReplica Count: 8\nBatch Size: 64\nLearning Rate: 2e-05\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The next section contains some plumbing code to get and combine the different json files of the dataset into a Pandas DataFrame.\n\nAlso the necessary code to create the Tensorflow Datasets is provided.","metadata":{"papermill":{"duration":0.031723,"end_time":"2021-12-07T22:16:24.552642","exception":false,"start_time":"2021-12-07T22:16:24.520919","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_dataset(df, max_len, tokenizer, batch_size, shuffle=False):\n    total_samples = df.shape[0]\n\n    # Placeholders input\n    input_ids, input_masks = [], []\n\n    # Placeholder output\n    labels = []\n\n    # Tokenize\n    for index, row in tqdm(zip(range(0, total_samples), df.iterrows()), total=total_samples):\n\n        # Get title and description as strings\n        text = row[1]['Tweet']\n        partisan = row[1]['Type of Claim']\n\n        # Encode\n        input_encoded = tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=max_len,\n            truncation=True,\n            padding='max_length'\n        )\n        input_ids.append(input_encoded['input_ids'])\n        input_masks.append(input_encoded['attention_mask'])\n#         ['entertainment' 'state' 'sports' 'national' 'kolkata' 'international']\n        labels.append(\n            0 if partisan == 'Simple' else\n            1 if partisan == 'Composite' else\n            2 if partisan == 'Compound' else None)\n\n    # Prepare and Create TF Dataset.\n    all_input_ids = tf.Variable(input_ids)\n    all_input_masks = tf.Variable(input_masks)\n    all_labels = tf.Variable(labels)\n    \n    dataset = tf.data.Dataset.from_tensor_slices(\n        (\n            {\n                'input_ids': all_input_ids,\n                'attention_mask': all_input_masks\n            },\n            all_labels\n        )\n    )\n    \n    if shuffle:\n        dataset = dataset.shuffle(64, reshuffle_each_iteration=True)\n        \n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-14T17:28:14.299931Z","iopub.execute_input":"2023-08-14T17:28:14.300310Z","iopub.status.idle":"2023-08-14T17:28:14.313033Z","shell.execute_reply.started":"2023-08-14T17:28:14.300281Z","shell.execute_reply":"2023-08-14T17:28:14.311935Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Models Code","metadata":{"papermill":{"duration":0.03181,"end_time":"2021-12-07T22:16:24.705836","exception":false,"start_time":"2021-12-07T22:16:24.674026","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"The following section contains the code for setting up the different models, saving the model files and a custom accuracy metric implementation for the mT5 and ByT5 models.","metadata":{"papermill":{"duration":0.031382,"end_time":"2021-12-07T22:16:24.769951","exception":false,"start_time":"2021-12-07T22:16:24.738569","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def ModelCheckpoint(model_name):\n    return tf.keras.callbacks.ModelCheckpoint(model_name, \n                                              monitor = 'val_accuracy', \n                                              verbose = 1, \n                                              save_best_only = True, \n                                              save_weights_only = True, \n                                              mode = 'max', \n                                              period = 1)\n\ndef create_distilmbert_model(model_type, strategy, config, lr):\n    # Create 'Standard' Classification Model\n    with strategy.scope():   \n        model = TFDistilBertForSequenceClassification.from_pretrained(model_type, config = config)\n        \n        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\n        model.compile(optimizer = optimizer, loss = loss, metrics = [metric])        \n        \n        return model\n\ndef create_gpt2_model(model_type, max_len, strategy, config, lr, tokenizer):\n    # NOTE! There is a TFGPT2ForSequenceClassification class available\n    # When using it I ran into some issues which where similar to some open issues on the\n    # Huggingface site. I will give this some more effort when I have the time available.\n    # Creating a Custom Model with TFGPT2Model just works...and does the same thing.\n    with strategy.scope():   \n        input_ids = tf.keras.layers.Input(shape = (max_len,), dtype = tf.int32, name = 'input_ids')\n        input_masks = tf.keras.layers.Input(shape = (max_len,), dtype = tf.int32, name = 'attention_mask')\n        \n        gpt2_model = TFGPT2Model.from_pretrained(model_type, config = config, from_pt = True)\n        gpt2_model.resize_token_embeddings(len(tokenizer))\n        gpt2_model.config.pad_token_id = gpt2_model.config.eos_token_id \n        \n        last_hidden_states = gpt2_model({'input_ids': input_ids, 'attention_mask': input_masks})\n        x = last_hidden_states[0][:, 0, :]\n        x = tf.keras.layers.Dropout(0.2)(x)\n        outputs = tf.keras.layers.Dense(6)(x)\n        model = tf.keras.Model(inputs = [input_ids, input_masks], outputs = outputs) \n\n        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\n        # Compile\n        model.compile(optimizer = optimizer, loss = loss, metrics = [metric])        \n        \n        return model\n    \ndef create_mbart_model(model_type, strategy, config, max_len, lr):\n    # Create 'Custom' Classification Model as we only have TFMBartModel\n    with strategy.scope():   \n        input_ids = tf.keras.layers.Input(shape = (max_len,), dtype = tf.int32, name = 'input_ids')\n        input_masks = tf.keras.layers.Input(shape = (max_len,), dtype = tf.int32, name = 'attention_mask')\n        \n        mbart_model = TFMBartModel.from_pretrained(model_type, config = config, from_pt = True)\n        \n        last_hidden_states = mbart_model({'input_ids': input_ids, 'attention_mask': input_masks})\n        x = last_hidden_states[0][:, 0, :]\n        x = tf.keras.layers.Dropout(0.2)(x)\n        outputs = tf.keras.layers.Dense(2)(x)\n        model = tf.keras.Model(inputs = [input_ids, input_masks], outputs = outputs) \n\n        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\n        # Compile\n        model.compile(optimizer = optimizer, loss = loss, metrics = [metric])        \n        \n        return model\n    \ndef create_mbert_model(model_type, strategy, config, lr):\n    # Create 'Standard' Classification Model\n    with strategy.scope():   \n        model = TFBertForSequenceClassification.from_pretrained(model_type, config = config)\n        \n        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\n        model.compile(optimizer = optimizer, loss = loss, metrics = [metric])        \n        \n        return model\n    \ndef create_xlm_roberta_model(model_type, strategy, config, lr):            \n    # Create 'Standard' Classification Model\n    with strategy.scope():   \n        model = TFXLMRobertaForSequenceClassification.from_pretrained(model_type, config = config)\n        \n        optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n\n        model.compile(optimizer = optimizer, loss = loss, metrics = [metric])        \n        \n        return model\n\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":0.077937,"end_time":"2021-12-07T22:16:24.879621","exception":false,"start_time":"2021-12-07T22:16:24.801684","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-14T17:28:18.680026Z","iopub.execute_input":"2023-08-14T17:28:18.680399Z","iopub.status.idle":"2023-08-14T17:28:18.709055Z","shell.execute_reply.started":"2023-08-14T17:28:18.680371Z","shell.execute_reply":"2023-08-14T17:28:18.708016Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Get Twitter Dataset\n\nNext it is time to create the news dataset that will be used for training and validation of the 4 models.\n\nIn the dataframe sample output below you can see the 'text' column that will be used as input text for each model. Also visible is the column 'partisan' that will be used as the label for which the models will learn to classify the input text.","metadata":{"papermill":{"duration":0.031542,"end_time":"2021-12-07T22:16:24.943836","exception":false,"start_time":"2021-12-07T22:16:24.912294","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pip install openpyxl\n","metadata":{"execution":{"iopub.status.busy":"2023-08-14T17:28:24.152093Z","iopub.execute_input":"2023-08-14T17:28:24.152463Z","iopub.status.idle":"2023-08-14T17:28:29.928613Z","shell.execute_reply.started":"2023-08-14T17:28:24.152435Z","shell.execute_reply":"2023-08-14T17:28:29.927249Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Requirement already satisfied: openpyxl in /usr/local/lib/python3.8/site-packages (3.1.2)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/site-packages (from openpyxl) (1.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndpgnews_df = pd.read_excel('/kaggle/input/data-train/English Anotated data.xlsx') ","metadata":{"execution":{"iopub.status.busy":"2023-08-14T17:28:29.930677Z","iopub.execute_input":"2023-08-14T17:28:29.931013Z","iopub.status.idle":"2023-08-14T17:28:30.443287Z","shell.execute_reply.started":"2023-08-14T17:28:29.930981Z","shell.execute_reply":"2023-08-14T17:28:30.442164Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"dpgnews_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T17:28:33.762178Z","iopub.execute_input":"2023-08-14T17:28:33.762926Z","iopub.status.idle":"2023-08-14T17:28:33.785212Z","shell.execute_reply.started":"2023-08-14T17:28:33.762888Z","shell.execute_reply":"2023-08-14T17:28:33.784172Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                    ID                                              Tweet  \\\n0  1669782054240689920  #IntelBrief: In addition to #AlQaeda &amp; ISK...   \n1  1669781942504389888  Afghanistan-based extremists spark terror fear...   \n\n                                      Selected Claim Type of Claim  \\\n0  In addition to #AlQaeda &amp; ISK, there are a...     Composite   \n1  Afghanistan-based extremists spark terror fear...        Simple   \n\n                                            Entities  Remark  \n0  AlQaeda, terrorist, groups, active, Afghanista...     NaN  \n1  Afghanistan, spark, fears, Central, Asia terro...     NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Tweet</th>\n      <th>Selected Claim</th>\n      <th>Type of Claim</th>\n      <th>Entities</th>\n      <th>Remark</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1669782054240689920</td>\n      <td>#IntelBrief: In addition to #AlQaeda &amp;amp; ISK...</td>\n      <td>In addition to #AlQaeda &amp;amp; ISK, there are a...</td>\n      <td>Composite</td>\n      <td>AlQaeda, terrorist, groups, active, Afghanista...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1669781942504389888</td>\n      <td>Afghanistan-based extremists spark terror fear...</td>\n      <td>Afghanistan-based extremists spark terror fear...</td>\n      <td>Simple</td>\n      <td>Afghanistan, spark, fears, Central, Asia terro...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"labels = dpgnews_df['Type of Claim'].unique()\n\nprint(labels)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T17:28:36.151726Z","iopub.execute_input":"2023-08-14T17:28:36.152774Z","iopub.status.idle":"2023-08-14T17:28:36.158990Z","shell.execute_reply.started":"2023-08-14T17:28:36.152731Z","shell.execute_reply":"2023-08-14T17:28:36.157786Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"['Composite' 'Simple' 'Compound']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create Train Test Split\ntrain_df, val_df = train_test_split(dpgnews_df, \n                                    stratify = dpgnews_df['Type of Claim'].values, \n                                    test_size = TEST_SIZE, \n                                    random_state = SEED)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T17:28:38.350556Z","iopub.execute_input":"2023-08-14T17:28:38.351546Z","iopub.status.idle":"2023-08-14T17:28:38.361074Z","shell.execute_reply.started":"2023-08-14T17:28:38.351503Z","shell.execute_reply":"2023-08-14T17:28:38.360073Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Multi-Lingual BERT\n\nThe first model we will put to the test is Multi-Lingual BERT. When released in 2018 BERT caused a small revolution by improving drastically the scores achieved on multiple NLP tasks. To review the paper use the following [link](https://arxiv.org/abs/1810.04805).\n\nMulti-Lingual BERT is the same model...however pre-trained on a large multi-lingual Wikipedia dataset containing the top 104 languages. The model was pre-trained on 2 objectives: Masked Language Modelling and Next Sentence Prediction.\n\nNote that we will train the model for 4 epochs only. With the size of the used dataset this is more than sufficient to make sure the model converges.","metadata":{"papermill":{"duration":0.033405,"end_time":"2021-12-07T22:16:49.215678","exception":false,"start_time":"2021-12-07T22:16:49.182273","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Multi-Lingual BERT Constants\nEPOCHS = 12\nmodel_type = 'bert-base-multilingual-cased'\n\n# Set Config\nconfig = AutoConfig.from_pretrained(model_type, num_labels = 3) \n# Set Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_type, add_prefix_space = False, do_lower_case = False)\n\n# Cleanup\ntf.keras.backend.clear_session()    \nif tpu is not None:\n    tf.tpu.experimental.initialize_tpu_system(tpu)\ngc.collect()\n\n# Create Train and Validation Datasets\ntrain_dataset = create_dataset(train_df, MAX_LEN, tokenizer, BATCH_SIZE, shuffle = True)\nvalidation_dataset = create_dataset(val_df, MAX_LEN, tokenizer, BATCH_SIZE, shuffle = False)\n\n# Steps\ntrain_steps = train_df.shape[0] // BATCH_SIZE\nval_steps = val_df.shape[0] // BATCH_SIZE\nprint(f'Train Steps: {train_steps}')\nprint(f'Val Steps: {val_steps}')\n\n# Create Model\nmodel_BERT = create_mbert_model(model_type, strategy, config, LR)\n\n# Model Summary\nprint(model_BERT.summary())\n\n# Fit Model\nhistory = model_BERT.fit(train_dataset,\n                    steps_per_epoch = train_steps,\n                    validation_data = validation_dataset,\n                    validation_steps = val_steps,\n                    epochs = EPOCHS, \n                    verbose = VERBOSE)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":199.97723,"end_time":"2021-12-07T22:20:09.22665","exception":false,"start_time":"2021-12-07T22:16:49.24942","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-14T17:11:13.237198Z","iopub.execute_input":"2023-08-14T17:11:13.237674Z","iopub.status.idle":"2023-08-14T17:12:38.358927Z","shell.execute_reply.started":"2023-08-14T17:11:13.237638Z","shell.execute_reply":"2023-08-14T17:12:38.357650Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n","output_type":"stream"},{"name":"stdout","text":"WARNING:tensorflow:TPU system local has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:TPU system local has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Finished initializing TPU system.\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Finished initializing TPU system.\n100%|██████████| 412/412 [00:00<00:00, 1619.94it/s]\n100%|██████████| 104/104 [00:00<00:00, 1566.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Steps: 6\nVal Steps: 1\n","output_type":"stream"},{"name":"stderr","text":"All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n\nSome layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"tf_bert_for_sequence_classification\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bert (TFBertMainLayer)      multiple                  177853440 \n                                                                 \n dropout_37 (Dropout)        multiple                  0         \n                                                                 \n classifier (Dense)          multiple                  2307      \n                                                                 \n=================================================================\nTotal params: 177,855,747\nTrainable params: 177,855,747\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/12\n","output_type":"stream"},{"name":"stderr","text":"2023-08-14 17:12:04.049430: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-08-14 17:12:04.903969: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"6/6 [==============================] - ETA: 0s - loss: 1.0603 - accuracy: 0.4479","output_type":"stream"},{"name":"stderr","text":"2023-08-14 17:12:10.712750: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-08-14 17:12:10.867931: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"6/6 [==============================] - 35s 875ms/step - loss: 1.0603 - accuracy: 0.4479 - val_loss: 0.9453 - val_accuracy: 0.5312\nEpoch 2/12\n6/6 [==============================] - 2s 360ms/step - loss: 0.9490 - accuracy: 0.5781 - val_loss: 0.8604 - val_accuracy: 0.6406\nEpoch 3/12\n6/6 [==============================] - 2s 350ms/step - loss: 0.8639 - accuracy: 0.6224 - val_loss: 0.8356 - val_accuracy: 0.6094\nEpoch 4/12\n6/6 [==============================] - 2s 361ms/step - loss: 0.7680 - accuracy: 0.6901 - val_loss: 0.7637 - val_accuracy: 0.7031\nEpoch 5/12\n6/6 [==============================] - 2s 350ms/step - loss: 0.6755 - accuracy: 0.7474 - val_loss: 0.7487 - val_accuracy: 0.6875\nEpoch 6/12\n6/6 [==============================] - 2s 356ms/step - loss: 0.6241 - accuracy: 0.7552 - val_loss: 0.8255 - val_accuracy: 0.6406\nEpoch 7/12\n6/6 [==============================] - 2s 363ms/step - loss: 0.5884 - accuracy: 0.7630 - val_loss: 0.9079 - val_accuracy: 0.6562\nEpoch 8/12\n6/6 [==============================] - 2s 368ms/step - loss: 0.5541 - accuracy: 0.7786 - val_loss: 0.9341 - val_accuracy: 0.6406\nEpoch 9/12\n6/6 [==============================] - 2s 354ms/step - loss: 0.5066 - accuracy: 0.7943 - val_loss: 0.8816 - val_accuracy: 0.6250\nEpoch 10/12\n6/6 [==============================] - 2s 362ms/step - loss: 0.4563 - accuracy: 0.8177 - val_loss: 0.9712 - val_accuracy: 0.6562\nEpoch 11/12\n6/6 [==============================] - 2s 356ms/step - loss: 0.4150 - accuracy: 0.8359 - val_loss: 0.9468 - val_accuracy: 0.5938\nEpoch 12/12\n6/6 [==============================] - 2s 360ms/step - loss: 0.3885 - accuracy: 0.8438 - val_loss: 0.9470 - val_accuracy: 0.5938\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install sentencepiece\n","metadata":{"execution":{"iopub.status.busy":"2023-08-14T16:48:32.694307Z","iopub.execute_input":"2023-08-14T16:48:32.694820Z","iopub.status.idle":"2023-08-14T16:48:38.985250Z","shell.execute_reply.started":"2023-08-14T16:48:32.694783Z","shell.execute_reply":"2023-08-14T16:48:38.983680Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Collecting sentencepiece\n  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentencepiece\nSuccessfully installed sentencepiece-0.1.99\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Make predictions on the test dataset\n# y_true_RoBERT = []\n# y_pred_RoBERT = []\n# # outputs_XLM_logit = []\n\n# for batch in test_dataset:\n#     inputs = batch[0]\n#     labels = batch[1]\n#     outputs_XLM = model_XLMRoBERTa.predict(inputs)\n#     predicted_labels = np.argmax(outputs_XLM.logits, axis=1)  # Modify this line\n    \n#     y_true_RoBERT.extend(labels.numpy().tolist())\n#     y_pred_RoBERT.extend(predicted_labels.tolist())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import classification_report\n\n# # Assuming you have true labels y_true and predicted labels y_pred\n# report = classification_report(y_true_RoBERT,y_pred_RoBERT)\n\n# print(report)\n\n# # https://mathweb.ucsd.edu/~bdriver/286-Spring2008/Lecture%20Notes/SDE20080401.pdf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import confusion_matrix\n\n# # Calculate confusion matrix\n# confusion_mat = confusion_matrix(true_labels, predicted_labels)\n\n# # Print the confusion matrix\n# print(\"Confusion Matrix:\")\n# print(confusion_mat)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After training the model we can view the performance on the validation set. Let's see what classification accuracy has been achieved.","metadata":{"papermill":{"duration":0.1873,"end_time":"2021-12-07T22:20:09.60414","exception":false,"start_time":"2021-12-07T22:20:09.41684","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from transformers import AutoConfig, AutoTokenizer\n\n\n# Rest of your code\n","metadata":{"execution":{"iopub.status.busy":"2023-08-14T17:30:28.409028Z","iopub.execute_input":"2023-08-14T17:30:28.409818Z","iopub.status.idle":"2023-08-14T17:30:29.287075Z","shell.execute_reply.started":"2023-08-14T17:30:28.409781Z","shell.execute_reply":"2023-08-14T17:30:29.286090Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"code","source":"# XLM-RoBERTa Constants\nEPOCHS = 3\nmodel_type = 'roberta-base'\n\n# Set Config\nconfig = AutoConfig.from_pretrained(model_type, num_labels = 3) # 2 labels because we do binary classification\n\n# Set Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_type, add_prefix_space = False, do_lower_case = False)\n\n# Cleanup\ntf.keras.backend.clear_session()    \nif tpu is not None:\n    tf.tpu.experimental.initialize_tpu_system(tpu)\ngc.collect()\n\n# Create Train and Validation Datasets\ntrain_dataset = create_dataset(train_df, MAX_LEN, tokenizer, BATCH_SIZE, shuffle = True)\nvalidation_dataset = create_dataset(val_df, MAX_LEN, tokenizer, BATCH_SIZE, shuffle = False)\n\n# Steps\ntrain_steps = train_df.shape[0] // BATCH_SIZE\nval_steps = val_df.shape[0] // BATCH_SIZE\nprint(f'Train Steps: {train_steps}')\nprint(f'Val Steps: {val_steps}')\n\n# Create Model\nmodel_XLMRoBERTa = create_xlm_roberta_model(model_type, strategy, config, LR)\n\n# Model Summary\nprint(model_XLMRoBERTa.summary())\n\n# Fit Model\nhistory = model_XLMRoBERTa.fit(train_dataset,\n                    steps_per_epoch = train_steps,\n                    validation_data = validation_dataset,\n                    validation_steps = val_steps,\n                    epochs = EPOCHS, \n                    verbose = VERBOSE)","metadata":{"execution":{"iopub.status.busy":"2023-08-14T17:28:48.634257Z","iopub.execute_input":"2023-08-14T17:28:48.635213Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nWARNING:tensorflow:TPU system local has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\nINFO:tensorflow:Initializing the TPU system: local\n","output_type":"stream"},{"name":"stderr","text":"free(): corrupted unsorted chunks\nhttps://symbolize.stripped_domain/r/?trace=79d9c0641ccc,79d9c05f2f8f,56949ce5248f&map= \n*** SIGABRT received by PID 9864 (TID 10680) on cpu 78 from PID 9864; stack trace: ***\nPC: @     0x79d9c0641ccc  (unknown)  (unknown)\n    @     0x79d8c4a087fa       1152  (unknown)\n    @     0x79d9c05f2f90      16528  (unknown)\n    @     0x56949ce52490  (unknown)  (unknown)\nhttps://symbolize.stripped_domain/r/?trace=79d9c0641ccc,79d8c4a087f9,79d9c05f2f8f,56949ce5248f&map=a5742613da17b2beb1178f2ea3f818c3:79d8b9600000-79d8c4c20be0 \nE0814 17:28:51.181303   10680 coredump_hook.cc:409] RAW: Remote crash data gathering hook invoked.\nE0814 17:28:51.181321   10680 client.cc:278] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.\nE0814 17:28:51.181324   10680 coredump_hook.cc:507] RAW: Sending fingerprint to remote end.\nE0814 17:28:51.181332   10680 coredump_socket.cc:120] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket\nE0814 17:28:51.181336   10680 coredump_hook.cc:513] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running?\nE0814 17:28:51.181340   10680 coredump_hook.cc:575] RAW: Dumping core locally.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Validation Performance\nprint(f'\\n===== Multi-Lingual BERT Classification Accuracy: {np.max(history.history[\"val_accuracy\"])*100:.3f}%')","metadata":{"papermill":{"duration":11.319434,"end_time":"2021-12-07T22:20:21.110686","exception":false,"start_time":"2021-12-07T22:20:09.791252","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-30T18:42:54.762682Z","iopub.execute_input":"2023-05-30T18:42:54.763165Z","iopub.status.idle":"2023-05-30T18:42:54.769616Z","shell.execute_reply.started":"2023-05-30T18:42:54.763121Z","shell.execute_reply":"2023-05-30T18:42:54.768633Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\n===== Multi-Lingual BERT Classification Accuracy: 93.980%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Multi-Lingual DistilBERT\n\nThe second model we will put to the test is Multi-Lingual DistilBERT. To review the paper use the following [link](https://arxiv.org/abs/1910.01108).\n\nMulti-Lingual DistilBERT is 40% smaller in size than BERT, 60% faster and retained 97% of the language understanding capabilities according to the paper summary.\n\nNote that we will train the model for 4 epochs only. With the size of the used dataset this is more than sufficient to make sure the model converges.","metadata":{}},{"cell_type":"code","source":"# Multi-Lingual DistilBERT Constants\nEPOCHS = 3\nmodel_type = 'distilbert-base-multilingual-cased'\n\n# Set Config\nconfig = AutoConfig.from_pretrained(model_type, num_labels = 3) \n# Set Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_type, add_prefix_space = False, do_lower_case = False)\n\n# Cleanup\ntf.keras.backend.clear_session()    \nif tpu is not None:\n    tf.tpu.experimental.initialize_tpu_system(tpu)\ngc.collect()\n\n# Create Train and Validation Datasets\ntrain_dataset = create_dataset(train_df, MAX_LEN, tokenizer, BATCH_SIZE, shuffle = True)\nvalidation_dataset = create_dataset(val_df, MAX_LEN, tokenizer, BATCH_SIZE, shuffle = False)\n\n# Steps\ntrain_steps = train_df.shape[0] // BATCH_SIZE\nval_steps = val_df.shape[0] // BATCH_SIZE\nprint(f'Train Steps: {train_steps}')\nprint(f'Val Steps: {val_steps}')\n\n# Create Model\nmodel = create_distilmbert_model(model_type, strategy, config, LR)\n\n# Model Summary\nprint(model.summary())\n\n# Fit Model\nhistory = model.fit(train_dataset,\n                    steps_per_epoch = train_steps,\n                    validation_data = validation_dataset,\n                    validation_steps = val_steps,\n                    epochs = EPOCHS, \n                    verbose = VERBOSE)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-14T17:30:33.747770Z","iopub.execute_input":"2023-08-14T17:30:33.748285Z","iopub.status.idle":"2023-08-14T17:30:35.311928Z","shell.execute_reply.started":"2023-08-14T17:30:33.748250Z","shell.execute_reply":"2023-08-14T17:30:35.310692Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Downloading (…)lve/main/config.json: 100%|██████████| 466/466 [00:00<00:00, 54.5kB/s]\nDownloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 4.04kB/s]\nDownloading (…)solve/main/vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 10.9MB/s]\nDownloading (…)/main/tokenizer.json: 100%|██████████| 1.96M/1.96M [00:00<00:00, 8.67MB/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_type, add_prefix_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, do_lower_case \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()    \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tpu \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     tf\u001b[38;5;241m.\u001b[39mtpu\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39minitialize_tpu_system(tpu)\n","\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"],"ename":"NameError","evalue":"name 'tf' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"After training the model we can view the performance on the validation set. Let's see what classification accuracy has been achieved.","metadata":{}},{"cell_type":"code","source":"# Validation Performance\nprint(f'\\n===== Multi-Lingual DistilBERT Classification Accuracy: {np.max(history.history[\"val_accuracy\"])*100:.3f}%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleanup\ndel model, train_dataset, validation_dataset\ngc.collect()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XLM-RoBERTa\n\nThe third model we will put to the test is XLM-RoBERTa. It is based on the earlier released RoBERTa model. In the paper it is mentioned that XLM-RoBERTa outperforms Multi-Lingual BERT on various tasks. To review the paper use the following [link](https://arxiv.org/abs/1911.02116).\n\nXLM-RoBERTa was pre-trained on 2.5TB of filtered text from the Common Crawl dataset. The dataset contains text for the top 100 languages. The model was pre-trained on 1 objective: Masked Language Modelling.\n\nNote that we will train the model for 4 epochs only. With the size of the used dataset this is more than sufficient to make sure the model converges.","metadata":{"papermill":{"duration":0.193412,"end_time":"2021-12-07T22:20:21.497967","exception":false,"start_time":"2021-12-07T22:20:21.304555","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"After training the model we can view the performance on the validation set. Let's see what classification accuracy has been achieved.","metadata":{"papermill":{"duration":0.361596,"end_time":"2021-12-07T22:24:15.743683","exception":false,"start_time":"2021-12-07T22:24:15.382087","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Validation Performance\nprint(f'\\n===== XLM-RoBERTa Classification Accuracy: {np.max(history.history[\"val_accuracy\"])*100:.3f}%')","metadata":{"execution":{"iopub.execute_input":"2021-12-07T22:24:16.481399Z","iopub.status.busy":"2021-12-07T22:24:16.480687Z","iopub.status.idle":"2021-12-07T22:24:39.715822Z","shell.execute_reply":"2021-12-07T22:24:39.716376Z"},"papermill":{"duration":23.611815,"end_time":"2021-12-07T22:24:39.716578","exception":false,"start_time":"2021-12-07T22:24:16.104763","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleanup\ndel model, train_dataset, validation_dataset\ngc.collect()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MBart\n\nThe fourth model we will put to the test is MBart. To review the paper use the following [link](https://arxiv.org/abs/2001.08210).\n\nNote that we will train the model for 4 epochs only. With the size of the used dataset this is more than sufficient to make sure the model converges.","metadata":{}},{"cell_type":"code","source":"# MBart Constants\nEPOCHS = 3\nmodel_type = 'facebook/mbart-large-cc25'\n\n# Set Config\nconfig = AutoConfig.from_pretrained(model_type, num_labels = 2) # 2 labels because we do binary classification\n\n# Set Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_type, add_prefix_space = False, do_lower_case = False)\n\n# Cleanup\ntf.keras.backend.clear_session()    \nif tpu is not None:\n    tf.tpu.experimental.initialize_tpu_system(tpu)\ngc.collect()\n\n# Create Train and Validation Datasets\ntrain_dataset = create_dataset(train_df, MAX_LEN, tokenizer, BATCH_SIZE, shuffle = True)\nvalidation_dataset = create_dataset(val_df, MAX_LEN, tokenizer, BATCH_SIZE, shuffle = False)\n\n# Steps\ntrain_steps = train_df.shape[0] // BATCH_SIZE\nval_steps = val_df.shape[0] // BATCH_SIZE\nprint(f'Train Steps: {train_steps}')\nprint(f'Val Steps: {val_steps}')\n\n# Create Model\nmodel = create_mbart_model(model_type, strategy, config, MAX_LEN, LR)\n\n# Model Summary\nprint(model.summary())\n\n# Fit Model\nhistory = model.fit(train_dataset,\n                    steps_per_epoch = train_steps,\n                    validation_data = validation_dataset,\n                    validation_steps = val_steps,\n                    epochs = EPOCHS, \n                    verbose = VERBOSE)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After training the model we can view the performance on the validation set. Let's see what classification accuracy has been achieved.","metadata":{}},{"cell_type":"code","source":"# Validation Performance\nprint(f'\\n===== MBart Classification Accuracy: {np.max(history.history[\"val_accuracy\"])*100:.3f}%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One 'issue' that we have with mT5 (and for ByT5...) is that it is a generative model. It generates text and doesn't have a Dense output layer as Multi-Lingual BERT or XLM-RoBERTa where we output a probability between 0 and 1 to predict the partisan label.\n\nWhat we can do however is generate 'text-labels' that present the classification label.\n\nSo we will train the mT5 and ByT5 models to predict the following 'text-labels':\n* Partisan label: True ==> mT5/ByT5 label to generate/classify as 'politiek'\n* Partisan label: False ==> mT5/ByT5 label to generate/classify as 'neutraal'\n\nBelow you can see how the labels are encoded and what their token values are.","metadata":{"papermill":{"duration":0.369036,"end_time":"2021-12-07T22:24:49.550375","exception":false,"start_time":"2021-12-07T22:24:49.181339","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Cleanup\ndel model, train_dataset, validation_dataset\ngc.collect()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After training the model we can view the performance on the validation set. Let's see what classification accuracy has been achieved.","metadata":{}},{"cell_type":"markdown","source":"## Summary and Results\n\nAfter training all the models and running the evaluation on the validation set we can see the achieved accuracy for each of the models.\n\nBelow an overview of the achieved accuracy scores (these scores are based on the previous version...so the scores can vary slightly because of the randomness involved...):\n1. MBart: xx%\n2. XLM-RoBERTa: xx%\n3. Multi-Lingual BERT: 67%\n4. Multi-Lingual DistilBERT: xx%\n\n\n","metadata":{"papermill":{"duration":0.85325,"end_time":"2021-12-07T22:48:30.716919","exception":false,"start_time":"2021-12-07T22:48:29.863669","status":"completed"},"tags":[],"_kg_hide-input":false}}]}